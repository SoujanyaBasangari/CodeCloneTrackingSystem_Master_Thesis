{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadec1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GetFiles\n",
    "import data_extraction\n",
    "import CloneDetector\n",
    "import cloneTracking\n",
    "\n",
    "print(\"Getting all file info from folder\")\n",
    "dirPath = \"/Users/vivekgoud/Documents/GitHub/Test_project_Codeclonetracer/onlinebookstore-J2EE/\"\n",
    "allFilesData= data_extraction.getAllFilesUsingFolderPath(dirPath)\n",
    "\n",
    "print(\"Extracting methods from files\",len(allFilesData),\"total_files\")\n",
    "\n",
    "current_dataset,linesofcode,codeclonelines= data_extraction.extractMethodsAllFiles(allFilesData)\n",
    "print(\"load transformed dataset to ML model\")\n",
    "\n",
    "total_files=len(allFilesData)\n",
    "\n",
    "#ml_dataset = cloneTracking.clonetracingModel(current_dataset)\n",
    "\n",
    "#cloning_percentage = (codeclonelines/linesofcode)*100\n",
    "\n",
    "#tracking_result = cloneTracking.analysis_creating_report(ml_dataset,total_files,cloning_percentage)\n",
    "\n",
    "print(\"check tracking.txt for latest report\")\n",
    "\n",
    "#print(linesofcode,\"total lines\",codeclonelines,\"total cloned lines\", (codeclonelines/linesofcode)*100 , \"cloning_percentage\")\n",
    "\n",
    "# CloneSave.writeToFile(codeBlocks)\n",
    "#CloneSave.writeToCSV(codeBlocks)\n",
    "\n",
    "\n",
    "#pip install python-Levenshtein\n",
    "\n",
    "#pip install pydriller\n",
    "#pip install fuzzywuzzy\n",
    "#pip install pandas\n",
    "#pip install javalang\n",
    "\n",
    "#pip install virtualenv\n",
    "\n",
    "# virtualenv ENV\n",
    "\n",
    "# source ENV/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1ce639",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chars2vecmodel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_extraction\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mCloneDetector\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchars2vecmodel\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chars2vecmodel'"
     ]
    }
   ],
   "source": [
    "import data_extraction\n",
    "import CloneDetector\n",
    "import chars2vecmodel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('//Users/vivekgoud/Documents/GitHub/CodeCloneTrackingSystem_Master_Thesis')\n",
    "\n",
    "import chars2vec\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5f87fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('//Users/vivekgoud/Documents/GitHub/CodeCloneTrackingSystem_Master_Thesis')\n",
    "\n",
    "import src.chars2vec\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import DistanceMetric\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.cluster.hierarchy as hcluster\n",
    "\n",
    "\n",
    "def cluster_indices(cluster_assignments):\n",
    "    n = cluster_assignments.max()\n",
    "    indices = []\n",
    "    for cluster_number in range(1, n + 1):\n",
    "        indices.append(np.where(cluster_assignments == cluster_number)[0])\n",
    "    return indices\n",
    "\n",
    "\n",
    "def clonetracingModel(df):\n",
    "    df = df.drop_duplicates(subset=['codeBlockId', 'Revision', 'codeCloneBlockId'], keep='last')\n",
    "    df[\"unique\"] = \"R1\" + df[\"Revision\"].astype(str) + df[\"codeBlockId\"]\n",
    "    df = df.reset_index(drop=True)\n",
    "    c2v_model = src.chars2vec.load_model('eng_300')\n",
    "\n",
    "    preprocessed_dataset = df[\n",
    "        ['codeBlockId', 'codeblock_Code', 'Revision', 'codeBlock_start', 'codeBlock_end', 'codeBlock_fileinfo',\n",
    "         \"unique\"]]\n",
    "\n",
    "    preprocessed_dataset = preprocessed_dataset.drop_duplicates(subset=['codeBlockId', 'Revision'], keep='last')\n",
    "\n",
    "    codeblock_Code = preprocessed_dataset['codeblock_Code'].tolist()\n",
    "    # Create word embeddings\n",
    "    codeblock_Code = c2v_model.vectorize_words(codeblock_Code)\n",
    "    preprocessed_dataset['emdedding_codeblock_Code'] = codeblock_Code.tolist()\n",
    "    data = preprocessed_dataset[['unique', 'emdedding_codeblock_Code']]\n",
    "    dist = DistanceMetric.get_metric('manhattan')  # manhattan euclidean\n",
    "\n",
    "    manhattan_distance_df = pd.DataFrame(\n",
    "        dist.pairwise(numpy.asarray([numpy.array(xi) for xi in data['emdedding_codeblock_Code']])),\n",
    "        columns=data.unique.unique(), index=data.unique.unique())\n",
    "\n",
    "    # clustering\n",
    "    thresh = 1.5\n",
    "    clusters = hcluster.fclusterdata(manhattan_distance_df, thresh, criterion=\"distance\")\n",
    "\n",
    "    data['clonesets'] = clusters\n",
    "\n",
    "    # Print the indices of the data points in each cluster.\n",
    "    num_clusters = clusters.max()\n",
    "    print(\"Total %d clonesets\" % num_clusters)\n",
    "    indices = cluster_indices(clusters)\n",
    "    for k, ind in enumerate(indices):\n",
    "        print(\"cloneset\", k + 1, \"is\", ind)\n",
    "    \n",
    "    with open('tracking_result.txt', 'w') as f:\n",
    "        for k, ind in enumerate(indices):\n",
    "            f.write(\"cloneset\", k + 1, \":\") \n",
    "            f.write('\\t',data.iloc[ind]['unique'].to_list())\n",
    "    \n",
    "    final_dataframe = pd.merge(data, df, on='unique', how='inner')\n",
    "\n",
    "    return final_dataframe\n",
    "\n",
    "    # scale(manhattan_distance_df)\n",
    "    # plt.figure(figsize=(50, 12))\n",
    "    # dend=hcluster.dendrogram(hcluster.linkage(manhattan_distance_df,method='ward'))\n",
    "\n",
    "\n",
    "def analysis_creating_report(final_dataframe, total_files, cloning_percentage):\n",
    "    output = final_dataframe[\n",
    "        ['unique', 'Revision', 'clonesets', 'codeBlockId', 'codeBlock_start', 'codeBlock_end', 'nloc',\n",
    "         'codeBlock_fileinfo', 'codeCloneBlockId']]\n",
    "    output = output.drop_duplicates(subset=['unique'], keep='last')\n",
    "    output = output.sort_values('Revision')\n",
    "\n",
    "    output['codeBlockId'] = output['codeBlockId'].str.replace('CodeBlock', '')\n",
    "    output[\"codeBlockId\"] = output[\"codeBlockId\"].astype(int)\n",
    "    output['Revision'] = output['Revision'].str.replace('R', '')\n",
    "    output[\"Revision\"] = output[\"Revision\"].astype(int)\n",
    "\n",
    "    idx = output.index\n",
    "\n",
    "    output.sort_values(['codeBlockId', 'Revision'], inplace=True)\n",
    "\n",
    "    output['codeBlock_start_diffs'] = output['codeBlock_start'].diff()\n",
    "    output['codeBlock_end_diff'] = output['codeBlock_end'].diff()\n",
    "    output['nloc_diff'] = output['nloc'].diff()\n",
    "\n",
    "    mask = output.codeBlockId != output.codeBlockId.shift(1)\n",
    "    output['codeBlock_start_diffs'][mask] = np.nan\n",
    "    output['codeBlock_end_diff'][mask] = np.nan\n",
    "    output['nloc_diff'][mask] = np.nan\n",
    "\n",
    "    output.sort_values(['Revision'], ascending=True, inplace=True)\n",
    "\n",
    "    output.reindex(idx)\n",
    "\n",
    "    output.sort_values([\"Revision\", \"codeBlockId\"], ascending=True).groupby(\"codeBlockId\").first()\n",
    "\n",
    "    output['ix'] = output.index\n",
    "    ix_first = output.sort_values([\"Revision\", \"codeBlockId\"], ascending=True).groupby(\"codeBlockId\").first()['ix']\n",
    "    output['status'] = ''\n",
    "    output['status'] = output['status'].where(output['ix'].isin(ix_first), 'stable')\n",
    "    output['status'] = output['status'].replace('', 'new')\n",
    "    output.loc[output.codeBlock_end_diff > 0, 'status'] = 'Modified/Added'\n",
    "    output.loc[output.codeBlock_end_diff < 0, 'status'] = 'Modified/removed'\n",
    "    output['codeBlock_start_diffs'] = output['codeBlock_start_diffs'].replace(np.NaN, 'new')\n",
    "    output['codeBlock_end_diff'] = output['codeBlock_end_diff'].replace(np.NaN, 'new')\n",
    "    output['nloc_diff'] = output['nloc_diff'].replace(np.NaN, 'new')\n",
    "    output = output.drop(columns=['ix'])\n",
    "    output['disappearing_clone'] = 3\n",
    "    output = output.set_index([\"Revision\", 'codeBlockId'])\n",
    "\n",
    "    index = pd.MultiIndex.from_product(output.index.levels, names=output.index.names)\n",
    "    output = output.reindex(index, fill_value=2).reset_index(level=1, drop=False).reset_index()\n",
    "\n",
    "    output.sort_values(['codeBlockId', 'Revision'], inplace=True)\n",
    "\n",
    "    idx = output.index\n",
    "    output['disappearing_clone_diffs'] = output['disappearing_clone'].diff()\n",
    "    mask = output.codeBlockId != output.codeBlockId.shift(1)\n",
    "    output['disappearing_clone_diffs'][mask] = np.nan\n",
    "    output['disappearing_clone_diffs'] = output['disappearing_clone_diffs'].replace(-1.0, 'disappearing_clone')\n",
    "    output = output.drop(columns=['disappearing_clone'])\n",
    "    output['disappearing_clone_diffs'] = output['disappearing_clone_diffs'].fillna('disappearing_clone')\n",
    "    output = output.drop(output[(output['disappearing_clone_diffs'] == 0.0) & (output['status'] == 2)].index)\n",
    "    output.reindex(idx)\n",
    "    output = output.sort_values('Revision')\n",
    "    \"\"\"\n",
    "    with open('tracking_result.txt', 'w') as f:\n",
    "        \n",
    "        f.write(\"cloning_percentage = \"+cloning_percentage + \"\\n\")\n",
    "\n",
    "        f.write(\"FILE LEVEL INFORMATION\")\n",
    "\n",
    "        f.write(\"total_files = \" + total_files + \"\\n\")\n",
    "        maxvalue=output['Revision'].max()\n",
    "        final_revision = output[output.Revision == 4]\n",
    "        f.write(\"final_revision = \" + final_revision + \"\\n\" )\n",
    "\n",
    "        maxvalue=output['Revision'].max()\n",
    "        final_revision = output[output.Revision == maxvalue]\n",
    "\n",
    "        files_containing_clones = len(pd.unique(final_revision['codeBlock_fileinfo']))#final_revision.codeBlock_fileinfo.count()\n",
    "        f.write(\"files_containing_clones\",files_containing_clones+ \"\\n\")\n",
    "\n",
    "\n",
    "        added_files = len(pd.unique(final_revision[final_revision['status']== 'new']['codeBlock_fileinfo']))#\n",
    "\n",
    "        f.write(\"added_files\",added_files+ \"\\n\")\n",
    "\n",
    "        deleted_files_df = final_revision[(final_revision.disappearing_clone_diffs == 'disappearing_clone') & (final_revision.status == 2)]\n",
    "\n",
    "        deleted_files =  len(pd.unique(deleted_files_df.codeBlock_fileinfo))\n",
    "\n",
    "        f.write(\"deleted_files\",deleted_files+ \"\\n\")\n",
    "\n",
    "        f.write(\"CLONESETS INFORMATION\")\n",
    "\n",
    "        total_clone_sets = len(pd.unique(final_revision.clonesets))\n",
    "\n",
    "        f.write(\"total_clone_sets\",total_clone_sets+ \"\\n\")\n",
    "\n",
    "        stable_clonesets = len(pd.unique(final_revision[final_revision['status']== 'stable']['clonesets']))\n",
    "\n",
    "        f.write(\"stable_clonesets\",stable_clonesets+ \"\\n\")\n",
    "\n",
    "        new_clonesets = len(pd.unique(final_revision[final_revision['status']== 'new']['clonesets']))\n",
    "\n",
    "        f.write(\"new_clonesets\",new_clonesets+ \"\\n\")\n",
    "\n",
    "        deleted_clonesets = len(pd.unique(final_revision[(final_revision.disappearing_clone_diffs == 'disappearing_clone') & (final_revision.status == 2)]['clonesets']))\n",
    "\n",
    "        f.write(\"deleted_clonesets\",deleted_clonesets+ \"\\n\")\n",
    "\n",
    "        final_revision['status']=final_revision['status'].astype(str)\n",
    "\n",
    "        changed_clonesets = len(pd.unique(final_revision[final_revision['status'].str.contains('Modified')]['clonesets']))\n",
    "\n",
    "        f.write(\"changed_clonesets\",changed_clonesets+ \"\\n\")\n",
    "\n",
    "        f.write(\"CODECLONES INFORMATION\")\n",
    "\n",
    "        total_codeclones= len(pd.unique(final_revision.codeBlockId))\n",
    "\n",
    "        f.write(\"total_codeclones\",total_codeclones+ \"\\n\")\n",
    "\n",
    "        stable_codeclones = len(pd.unique(final_revision[final_revision['status']== 'stable']['codeBlockId']))\n",
    "\n",
    "        f.write(\"stable_codeclones\",stable_codeclones+ \"\\n\")\n",
    "\n",
    "        new_codeclones = len(pd.unique(final_revision[final_revision['status']== 'new']['codeBlockId']))\n",
    "\n",
    "        f.write(\"new_codeclones\",new_codeclones+ \"\\n\")\n",
    "\n",
    "        deleted_codeclones = len(pd.unique(final_revision[(final_revision.disappearing_clone_diffs == 'disappearing_clone') & (final_revision.status == 2)]['codeBlockId']))\n",
    "\n",
    "        f.write(\"deleted_codeclones\",deleted_codeclones+ \"\\n\")\n",
    "\n",
    "        changed_codeclones = len(pd.unique(final_revision[final_revision['status'].str.contains('Modified')]['codeBlockId']))\n",
    "\n",
    "        f.write(\"changed_codeclones\",changed_codeclones+ \"\\n\")\n",
    "\n",
    "        f.write('\\n' )\n",
    "        f.close()\n",
    "    \"\"\"\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac7ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
