{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0957d60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import Mapping\n",
    "import DataFlowApproach\n",
    "import Config\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def getFrequencyFromList(lst):\n",
    "    dict_freq = {}\n",
    "    for token in lst:\n",
    "        if token in dict_freq.keys():\n",
    "            dict_freq[token] = dict_freq[token] + 1\n",
    "        else:\n",
    "            dict_freq[token] = 1\n",
    "\n",
    "    return dict_freq\n",
    "\n",
    "\n",
    "def getMostFrequent(dict_freq, threshold=1):\n",
    "    lst_token_freq = sorted(\n",
    "        dict_freq.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    lst_token = []\n",
    "    for idx in range(math.ceil(len(lst_token_freq)*threshold)):\n",
    "        if idx >= len(lst_token_freq):\n",
    "            break\n",
    "        lst_token.append(lst_token_freq[idx][0])\n",
    "    return lst_token\n",
    "\n",
    "\n",
    "def detectClone(codeBlocks):\n",
    "    for codeBlockId in codeBlocks:\n",
    "        codeBlock = codeBlocks[codeBlockId]\n",
    "        code = codeBlock['Code']\n",
    "        dict_tokens, dict_variables, dict_methods = getAllTokens(code)\n",
    "\n",
    "        # print(\"dict_v \", dict_variables)\n",
    "        # print(\"dict_mc\", dict_methods)\n",
    "        # print(\"dict_t\", dict_tokens)\n",
    "        variables_lst = getMostFrequent(\n",
    "            dict_variables, Config.variableAndMethodsThreshold)\n",
    "        methods_lst = getMostFrequent(\n",
    "            dict_methods, Config.variableAndMethodsThreshold)\n",
    "\n",
    "        # print(\"identifiers \", variables_lst)\n",
    "        # print(\"methods \", methods_lst)\n",
    "        variable_scope, method_calls_scope = DataFlowApproach.dataFlowGenerator(\n",
    "            code, variables_lst, methods_lst, [codeBlock['FileInfo'], codeBlock['Start'], codeBlock['End']])\n",
    "\n",
    "        # print(\"VC\", variable_scope)\n",
    "        # print(\"MCS \", method_calls_scope)\n",
    "        codeBlock.update({\"Tokens\": dict_tokens})\n",
    "        codeBlock.update({\"Variables_Scope\": variable_scope})\n",
    "        codeBlock.update({\"Method_Calls_Scope\": method_calls_scope})\n",
    "    \n",
    "    codeclonelines = 0\n",
    "\n",
    "    for codeBlockId in codeBlocks:\n",
    "        codeBlock = codeBlocks[codeBlockId]\n",
    "\n",
    "        tokens = codeBlock[\"Tokens\"]\n",
    "        variable_scope = codeBlock[\"Variables_Scope\"]\n",
    "        method_calls_scope = codeBlock[\"Method_Calls_Scope\"]\n",
    "\n",
    "        codeCloneIds = []\n",
    "        \n",
    "        for codeCandidateId in codeBlocks:\n",
    "            if codeCandidateId == codeBlockId:\n",
    "                continue\n",
    "\n",
    "            simTokens = similarity(\n",
    "                tokens, codeBlocks[codeCandidateId][\"Tokens\"])\n",
    "            if simTokens >= Config.tokenSimilarityThreshold:\n",
    "                # We will check the control flow of variables here\n",
    "                codeCandidateBlock = codeBlocks[codeCandidateId]\n",
    "                candidate_variable_scope = codeCandidateBlock[\"Variables_Scope\"]\n",
    "                candidate_method_calls_scope = codeCandidateBlock[\"Method_Calls_Scope\"]\n",
    "                # print(\"Variables Scope\", variable_scope)\n",
    "                # print(\"Methods Calls Scope\", method_calls_scope)\n",
    "                # print(\"Candidate vC\", candidate_variable_scope)\n",
    "                # print(\"Can MC\", candidate_method_calls_scope)\n",
    "                variableSimilarityByDataFlow, methodCallSimilarityByDataFlow = DataFlowApproach.getSimilarity(\n",
    "                    variable_scope, method_calls_scope, candidate_variable_scope, candidate_method_calls_scope,\n",
    "                    [codeBlock['FileInfo'], codeBlock['Start'], codeBlock['End'],\n",
    "                     codeCandidateBlock['FileInfo'], codeCandidateBlock['Start'], codeCandidateBlock['End']])\n",
    "                if variableSimilarityByDataFlow >= Config.similarityDataFlowThreshold and methodCallSimilarityByDataFlow >= Config.similarityDataFlowThreshold:\n",
    "                    codeclonelines = codeclonelines + len(codeCandidateBlock['Code'])\n",
    "                    codeCloneIds.append(\n",
    "                        {\"Similarity\": [simTokens, variableSimilarityByDataFlow, methodCallSimilarityByDataFlow], \"codeCandidateId\": codeCandidateId})\n",
    "        \n",
    "        codeBlock.update({\"CodeClones\": codeCloneIds})\n",
    "    \n",
    "    return codeBlocks,codeclonelines\n",
    "\n",
    "\n",
    "def getAllTokens(code):\n",
    "    list_methods = []\n",
    "    list_tokens = []\n",
    "    list_variables = []\n",
    "    for line in code:\n",
    "        line = re.sub(r\"(\\\".*?\\\"|\\'.*?\\')\", \" STRING_LITERAL \", line)\n",
    "        regexPattern = '|'.join(map(re.escape, Mapping.delimiters))\n",
    "        list_line = re.sub('(?<=\\W|\\w)(' + regexPattern + ')',\n",
    "                           r' \\1 ', line).split()\n",
    "        list_line = [unit.strip() for unit in list_line if unit.strip() != \"\"]\n",
    "        # print(list_line)\n",
    "\n",
    "        for idx in range(len(list_line)):\n",
    "            unit = list_line[idx].strip()\n",
    "            unit = re.sub(r\"^[+-]?((\\d*(\\.\\d*)?)|(\\.\\d*))$\",\n",
    "                          \"INTEGER_LITERAL\", unit)\n",
    "            if unit in Mapping.symbols:\n",
    "                continue\n",
    "            elif unit in Mapping.keywords.keys():\n",
    "                list_tokens.append(Mapping.keywords[unit])\n",
    "            else:\n",
    "                if idx + 1 < len(list_line) and list_line[idx + 1].strip() == '(':\n",
    "\n",
    "                    list_methodName = unit.split(\".\")\n",
    "\n",
    "                    list_methods.append(list_methodName[-1])\n",
    "\n",
    "                    list_tokens.append(list_methodName[-1])\n",
    "                    # list_tokens.append(\"TOKEN_METHOD\")\n",
    "\n",
    "                else:\n",
    "                    list_variableName = unit.split('.')\n",
    "                    # print(list_variableName)\n",
    "\n",
    "                    list_variables.append(list_variableName[-1])\n",
    "                    list_tokens.append(\"TOKEN_VARIABLE\")\n",
    "\n",
    "    dict_tokens =getFrequencyFromList(list_tokens)\n",
    "    dict_variables =getFrequencyFromList(list_variables)\n",
    "    dict_methods = getFrequencyFromList(list_methods)\n",
    "\n",
    "    return dict_tokens, dict_variables, dict_methods\n",
    "\n",
    "\n",
    "def similarity(Tokens1, Tokens2):\n",
    "    \"\"\"\n",
    "    input : two list of code\n",
    "    output : similarity between two list of tokens(decimal between 0 and 1)\n",
    "    \"\"\"\n",
    "    tokensIntersect = 0\n",
    "    tokens1 = 0\n",
    "    tokens2 = 0\n",
    "    tokensUnion = 0\n",
    "    Tokens1Keys = Tokens1.keys()\n",
    "    Tokens2Keys = Tokens2.keys()\n",
    "    for key in Tokens1Keys:\n",
    "        if key in Tokens2Keys:\n",
    "            tokensIntersect += min(Tokens1[key], Tokens2[key])\n",
    "    for key in Tokens1Keys:\n",
    "        tokens1 += Tokens1[key]\n",
    "    for key in Tokens2Keys:\n",
    "        tokens2 += Tokens2[key]\n",
    "    return (tokensIntersect)/(tokens1 + tokens2 - tokensIntersect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcdb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirPath = \"/Users/vivekgoud/Documents/GitHub/Test_project_Codeclonetracer\"\n",
    "#dirPath = \"F:\\8th-Sem-Project\\src\\examples\\Single\"\n",
    "#outputPath = \"F:\\8th-Sem-Project\\src\\CodeCloneDetection\\output.txt\"\n",
    "#outputCSVPath = \"F:\\8th-Sem-Project\\src\\CodeCloneDetection\\clonesDetected.csv\"\n",
    "# This will be used as level for output into file\n",
    "# 0 means everything\n",
    "# 1 means current block's code and only clone blocks info\n",
    "# 2 means only current block's and clone block's info\n",
    "outputLevel = 2\n",
    "\n",
    "# Minimum length of block to consider\n",
    "minimumLengthBlock = 4\n",
    "\n",
    "# Threshhold for considering as code clones\n",
    "# Threshhold = 1 for type 2 clones\n",
    "tokenSimilarityThreshold = 0.75\n",
    "\n",
    "# Threshold for similarity measure by data flow approach\n",
    "similarityDataFlowThreshold = 0.65\n",
    "\n",
    "# Threshold for considering most frequent variables and methods\n",
    "variableAndMethodsThreshold = 0.65\n",
    "\n",
    "# Threshold while comparing dataflow of two variables and methods\n",
    "dataFlowSimilaritythreshold = 0.65\n",
    "\n",
    "# Block level can be 0 = (file level) or 1 = (method level)\n",
    "granularity = 'method_level'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7be39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "#import GetFunctions\n",
    "import re\n",
    "import sys\n",
    "import traceback\n",
    "import CloneDetector\n",
    "import javalang\n",
    "\n",
    "import Config\n",
    "import pandas as pd\n",
    "global found_parent\n",
    "\n",
    "def extractMethodsAllFiles(listOfFiles):\n",
    "   \n",
    "    allFilesMethodsBlocks = {}\n",
    "    blocksSoFar = 0\n",
    "    linesofcode = 0\n",
    "    codeBlocks= {}\n",
    "    \n",
    "    for filePath in listOfFiles:\n",
    "        file = open(filePath, 'r', encoding='utf-8')\n",
    "        originalCode = file.readlines()\n",
    "        file.close()\n",
    "        if Config.granularity == 'method_level':\n",
    "      \n",
    "            linesofcode = linesofcode + len(originalCode)\n",
    "            codeBlocks = methodLevelBlocks(originalCode)\n",
    "            \n",
    "        else:\n",
    "            linesofcode = linesofcode + len(originalCode)\n",
    "            codeBlocks = fileLevelBlocks(originalCode)\n",
    "        if len(codeBlocks) == 0:\n",
    "            continue\n",
    "        for codeBlock in codeBlocks:\n",
    "            if len(codeBlock) == 0:\n",
    "                continue\n",
    "            codeBlock.update({\"FileInfo\": filePath})\n",
    "            codeBlock.update({\"nloc\": len(codeBlock)})\n",
    "            blocksSoFar += 1\n",
    "            allFilesMethodsBlocks[\"CodeBlock\" + str(blocksSoFar)] = codeBlock\n",
    "    print(\"detecting clones\")\n",
    "    granularity = Config.granularity\n",
    "    codeBlocks,codeclonelines=CloneDetector.detectClone(allFilesMethodsBlocks)\n",
    "   \n",
    "    previous_file_name = '/Users/vivekgoud/Downloads/thesis/Tracking_dataset.csv'\n",
    "    current_dataset=dataset_creation(codeBlocks)\n",
    "    \n",
    "    previous_dataset = pd.DataFrame()\n",
    "    previous_clones = pd.DataFrame(columns=['codeBlockId','codeBlock_start','codeBlock_end','codeBlock_fileinfo','codeblock_Code','codeCloneBlockId',\n",
    "                               'codeCloneBlock_Fileinfo','Similarity_Tokens','Similarity_Variable_Flow',\n",
    "                               'Similarity_MethodCall_Flow','commitinfo','nloc','Revision'])\n",
    "    if os.path.isfile(previous_file_name): #previous_file_name.exists(): \n",
    "        previous_dataset =  pd.read_csv(previous_file_name, index_col=0)\n",
    "        revision = previous_dataset.Revision.unique()\n",
    "        print(\"Revision\",revision[0])\n",
    "        previous_clones = previous_dataset[~previous_dataset.codeBlock_fileinfo.isin(current_dataset.codeBlock_fileinfo)]\n",
    "        frames = [current_dataset,previous_clones]\n",
    "        current_dataset=pd.concat([current_dataset,previous_clones])\n",
    "        current_dataset= current_dataset.loc[current_dataset.astype(str).drop_duplicates().index]\n",
    "        current_dataset['Revision'] = revision[0] + 1\n",
    "    else:\n",
    "        print(\"First version, no cloning result exists\")\n",
    "        current_dataset['Revision'] = 1\n",
    "\n",
    "    current_dataset = current_dataset.convert_dtypes()\n",
    "    all_columns = list(current_dataset) # Creates list of all column headers\n",
    "    current_dataset[all_columns] = current_dataset[all_columns].astype(str)\n",
    "    current_dataset= current_dataset.loc[current_dataset.astype(str).drop_duplicates().index]\n",
    "    current_dataset.to_csv('/Users/vivekgoud/Downloads/thesis/Tracking_dataset.csv')\n",
    "        #current_dataset.to_sql('rxjava', con= engine, if_exists='append', index=False)\n",
    "        #pd.read_sql('select count(*) from rxjava', conn=engine)\n",
    "        #current_dataset.to_sql('training_onlinebookstore', con=engine, if_exists='append', index=False)\"\"\"\n",
    "   \n",
    "    return current_dataset,linesofcode,codeclonelines\n",
    "\n",
    "def dataset_creation(codeBlocks):\n",
    "\n",
    "    df = pd.DataFrame(columns=['codeBlockId','codeBlock_start','codeBlock_end','codeBlock_fileinfo','codeblock_Code','codeCloneBlockId',\n",
    "                               'codeCloneBlock_Fileinfo','Similarity_Tokens','Similarity_Variable_Flow',\n",
    "                             'Similarity_MethodCall_Flow','commitinfo','nloc'])\n",
    "\n",
    "    output=[]\n",
    "    for codeBlockId in codeBlocks:\n",
    "          codeBlock = codeBlocks[codeBlockId]\n",
    "          for codeCloneBlockData in codeBlock[\"CodeClones\"]:\n",
    "            codeCloneBlockId = codeCloneBlockData[\"codeCandidateId\"]\n",
    "            codeCloneBlock = codeBlocks[codeCloneBlockId]\n",
    "            codeCloneSimilarity = codeCloneBlockData[\"Similarity\"]\n",
    "            output.append([codeBlockId,str(codeBlock[\"Start\"]),str(codeBlock[\"End\"]),codeBlock[\"FileInfo\"],codeBlock[\"Code\"],codeCloneBlockData[\"codeCandidateId\"],\n",
    "                       codeCloneBlock[\"FileInfo\"],str(codeCloneSimilarity[0]),str(codeCloneSimilarity[1]),\n",
    "                      str(codeCloneSimilarity[2]),str(codeBlock[\"nloc\"])\n",
    "                      ])            \n",
    "    for index,x in enumerate(output):\n",
    "        a_row=pd.Series([x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[11]],\n",
    "          index=['codeBlockId','codeBlock_start','codeBlock_end','codeBlock_fileinfo','codeblock_Code','codeCloneBlockId',\n",
    "                               'codeCloneBlock_Fileinfo','Similarity_Tokens','Similarity_Variable_Flow',\n",
    "                             'Similarity_MethodCall_Flow','commitinfo','nloc'])\n",
    "        df=pd.concat([df,a_row])\n",
    "        #row_df = pd.DataFrame([a_row])\n",
    "        #df=df.append(row_df) \n",
    "\n",
    "    return df\n",
    "\n",
    "def fileLevelBlocks(originalCode):\n",
    "    \"\"\"\n",
    "    input : originalCode\n",
    "    output : blocks using file level\n",
    "    \"\"\"\n",
    "\n",
    "    allCodeBlocks = []\n",
    "    commentsRemovedCode = removeCommentsFromCode(originalCode)\n",
    "    startLine = 1\n",
    "    endLine = len(commentsRemovedCode)\n",
    "    allCodeBlocks.append(\n",
    "        {\"Start\": startLine, \"End\": endLine, \"Code\": originalCode})\n",
    "    return allCodeBlocks\n",
    "\n",
    "\n",
    "def methodLevelBlocks(originalCode):\n",
    "    \"\"\"\n",
    "    input : originalCode\n",
    "    output : blocks using method level\n",
    "    \"\"\"\n",
    "    commentsRemovedCode = removeCommentsFromCode(originalCode)\n",
    "    codeInSingleLine = \"\\n\".join(commentsRemovedCode)\n",
    "\n",
    "    output = method_extractor(codeInSingleLine)\n",
    "\n",
    "    allCodeBlocks = []\n",
    "    if output[0] == None:\n",
    "        return allCodeBlocks\n",
    "    for i in range(len(output[0])):\n",
    "        if abs(output[0][i][1] - output[0][i][0]) < Config.minimumLengthBlock - 1:\n",
    "            continue\n",
    "        allCodeBlocks.append(\n",
    "            {\"Start\": output[0][i][0], \"End\": output[0][i][1], \"Code\": originalCode})# output[1][i].split('\\n')})\n",
    "    \n",
    "    return allCodeBlocks\n",
    "# get all lines of code before detection \n",
    "# get all clone code lines\n",
    "# send code blocks to dataset creation\n",
    "\n",
    "def removeCommentsFromCode(originalCode):\n",
    "    \"\"\"\n",
    "    input : original Code\n",
    "    output : code without comments \n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT = 1\n",
    "    ESCAPE = 2\n",
    "    STRING = 3\n",
    "    ONE_LINE_COMMENT = 4\n",
    "    MULTI_LINE_COMMENT = 5\n",
    "\n",
    "    mode = DEFAULT\n",
    "    strippedCode = []\n",
    "    for line in originalCode:\n",
    "        strippedLine = \"\"\n",
    "        idx = 0\n",
    "        while idx < len(line):\n",
    "            subString = line[idx: min(idx + 2, len(line))]\n",
    "            c = line[idx]\n",
    "            if mode == DEFAULT:\n",
    "                mode = MULTI_LINE_COMMENT if subString == \"/*\" else ONE_LINE_COMMENT if subString == \"//\" else STRING if c == '\\\"' else DEFAULT\n",
    "            elif mode == STRING:\n",
    "                mode = DEFAULT if c == '\\\"' else ESCAPE if c == '\\\\' else STRING\n",
    "            elif mode == ESCAPE:\n",
    "                mode = STRING\n",
    "            elif mode == ONE_LINE_COMMENT:\n",
    "                mode = DEFAULT if c == '\\n' else ONE_LINE_COMMENT\n",
    "                idx += 1\n",
    "                continue\n",
    "            elif mode == MULTI_LINE_COMMENT:\n",
    "                mode = DEFAULT if subString == \"*/\" else MULTI_LINE_COMMENT\n",
    "                idx += 2 if mode == DEFAULT else 1\n",
    "                continue\n",
    "            strippedLine += c if mode < 4 else \"\"\n",
    "            idx += 1\n",
    "        if len(strippedLine) > 0 and strippedLine[-1] == '\\n':\n",
    "            strippedLine = strippedLine[:-1]\n",
    "        # strippedLine = re.sub('\\t| +', ' ', strippedLine)\n",
    "        strippedCode.append(strippedLine)\n",
    "    return strippedCode\n",
    "\n",
    "try:\n",
    "    from configparser import ConfigParser\n",
    "except ImportError:\n",
    "    from ConfigParser import ConfigParser  # ver. < 3.0\n",
    "\n",
    "\n",
    "re_string = re.escape(\"\\\"\") + '.*?' + re.escape(\"\\\"\")\n",
    "\n",
    "\n",
    "def getFunctions(filestring, comment_inline_pattern=\".*?$\"):\n",
    "\n",
    "    method_string = []\n",
    "    method_pos = []\n",
    "    method_name = []\n",
    "\n",
    "    global found_parent\n",
    "    found_parent = []\n",
    "\n",
    "    tree = None\n",
    "\n",
    "    try:\n",
    "        tree = javalang.parse.parse(filestring)\n",
    "        package = tree.package\n",
    "        if package is None:\n",
    "            package = 'DefaultPackage'\n",
    "        else:\n",
    "            package = package.name\n",
    "            # print package,'####'\n",
    "    except Exception as e:\n",
    "        # logging.warning('Traceback:' + traceback.print_exc())\n",
    "        return (None, None, [])\n",
    "\n",
    "    file_string_split = filestring.split('\\n')\n",
    "    # print(file_string_split)\n",
    "    nodes = itertools.chain(tree.filter(\n",
    "        javalang.tree.ConstructorDeclaration), tree.filter(javalang.tree.MethodDeclaration))\n",
    "\n",
    "    for path, node in nodes:\n",
    "        # print(type(node))\n",
    "        # print '---------------------------------------'\n",
    "        name = '.'+node.name\n",
    "        for i, var in enumerate(reversed(path)):\n",
    "            # print var, i, len(path)-3\n",
    "            if isinstance(var, javalang.tree.ClassDeclaration):\n",
    "                # print 'One Up:',var,var.name\n",
    "                if len(path)-3 == i:  # Top most\n",
    "                    name = '.'+var.name+check_repetition(var, var.name)+name\n",
    "                else:\n",
    "                    name = '$'+var.name+check_repetition(var, var.name)+name\n",
    "            if isinstance(var, javalang.tree.ClassCreator):\n",
    "                # print 'One Up:',var,var.type.name\n",
    "                name = '$'+var.type.name + \\\n",
    "                    check_repetition(var, var.type.name)+name\n",
    "            if isinstance(var, javalang.tree.InterfaceDeclaration):\n",
    "                # print 'One Up:',var,var.name\n",
    "                name = '$'+var.name+check_repetition(var, var.name)+name\n",
    "        # print i,var,len(path)\n",
    "        # print path\n",
    "        # while len(path) != 0:\n",
    "        #  print path[:-1][-1]\n",
    "        args = []\n",
    "        for t in node.parameters:\n",
    "            dims = []\n",
    "            if len(t.type.dimensions) > 0:\n",
    "                for e in t.type.dimensions:\n",
    "                    dims.append(\"[]\")\n",
    "            dims = \"\".join(dims)\n",
    "            args.append(t.type.name+dims)\n",
    "        args = \",\".join(args)\n",
    "\n",
    "        fqn = (\"%s%s(%s)\") % (package, name, args)\n",
    "        # print \"->\",fqn\n",
    "\n",
    "        (init_line, b) = node.position\n",
    "        method_body = []\n",
    "        closed = 0\n",
    "        openned = 0\n",
    "\n",
    "        # print '###################################################################################################'\n",
    "        # print (init_line,b)\n",
    "        # print 'INIT LINE -> ',file_string_split[init_line-1]\n",
    "        # print '---------------------'\n",
    "\n",
    "        for line in file_string_split[init_line-1:]:\n",
    "            # if len(line) == 0:\n",
    "            #     continue\n",
    "            # print '+++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "            # print line\n",
    "            # print comment_inline_pattern\n",
    "            line_re = re.sub(comment_inline_pattern, '',\n",
    "                             line, flags=re.MULTILINE)\n",
    "            line_re = re.sub(re_string, '', line_re, flags=re.DOTALL)\n",
    "\n",
    "            # print line\n",
    "            # print '+++++++++++++++++++++++++++++++++++++++++++++++++++'\n",
    "\n",
    "            closed += line_re.count('}')\n",
    "            openned += line_re.count('{')\n",
    "            if (closed - openned) == 0 and openned > 0:\n",
    "                method_body.append(line)\n",
    "                break\n",
    "            else:\n",
    "                method_body.append(line)\n",
    "\n",
    "        # print '\\n'.join(method_body)\n",
    "\n",
    "        end_line = init_line + len(method_body) - 1\n",
    "        method_body = '\\n'.join(method_body)\n",
    "\n",
    "        method_pos.append((init_line, end_line))\n",
    "        method_string.append(method_body)\n",
    "\n",
    "        method_name.append(fqn)\n",
    "\n",
    "    if (len(method_pos) != len(method_string)):\n",
    "        # logging.warning(\"File \" + file_path + \" cannot be parsed. (3)\")\n",
    "        return (None, None, method_name)\n",
    "    else:\n",
    "        # logging.warning(\"File \" + file_path + \" successfully parsed.\")\n",
    "        return (method_pos, method_string, method_name)\n",
    "\n",
    "\n",
    "def check_repetition(node, name):\n",
    "    before = -1\n",
    "    i = 0\n",
    "    for (obj, n, value) in found_parent:\n",
    "        if obj is node:\n",
    "            if value == -1:\n",
    "                return ''\n",
    "            else:\n",
    "                return '_'+str(value)\n",
    "        else:\n",
    "            i += 1\n",
    "        if n == name:\n",
    "            before += 1\n",
    "    found_parent.append((node, name, before))\n",
    "    if before == -1:\n",
    "        return ''\n",
    "    else:\n",
    "        return '_'+str(before)\n",
    "\n",
    "\n",
    "def method_extractor(file):\n",
    "    methodsInfo = []\n",
    "\n",
    "    FORMAT = '[%(levelname)s] (%(threadName)s) %(message)s'\n",
    "    # logging.basicConfig(level=logging.DEBUG, format=FORMAT)\n",
    "\n",
    "    config = ConfigParser()\n",
    "\n",
    "    # parse existing file\n",
    "    #try:\n",
    "     #   config.read(os.path.join(os.path.dirname(\n",
    "      #      os.path.abspath(__file__)), 'config.ini'))\n",
    "    #except IOError:\n",
    "     #   print('ERROR - Config settings not found. Usage: $python this-script.py config-file.ini')\n",
    "      #  sys.exit()\n",
    "  \n",
    "    separators = \"; . [ ] ( ) ~ ! - + & * / % < > ^ | ? { } = # , \\\" \\\\ : $ ' ` @\"\n",
    "    comment_inline = \"#\"\n",
    "    comment_inline_pattern = comment_inline + '.*?$'\n",
    "\n",
    "    return getFunctions(file, comment_inline_pattern)\n",
    "\n",
    "    # allFilesInFolder = GetFiles.getAllFilesUsingFolderPath(folderPath)\n",
    "\n",
    "    # print(allFilesInFolder)\n",
    "\n",
    "\n",
    "def getAllFilesUsingFolderPath(folderPath):\n",
    "    allFilesInFolder = []\n",
    "    fileCount = 0\n",
    "    maxCount = 100\n",
    "    for subdir, dirs, files in os.walk(folderPath):\n",
    "        for fileName in files:\n",
    "            fileCount += 1\n",
    "            if fileName.split(\".\")[-1] != \"java\":\n",
    "                continue\n",
    "            fileFullPath = os.path.join(subdir, fileName)\n",
    "            allFilesInFolder.append(fileFullPath)\n",
    "            if fileCount > maxCount:\n",
    "                break\n",
    "    return allFilesInFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "626e1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Mapping\n",
    "\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import Config\n",
    "\n",
    "cf_mapping = {\"while\": \"iteration\",\n",
    "              \"for\": \"iteration\",\n",
    "              \"do\": \"iteration\",\n",
    "              \"if\": \"selection\",\n",
    "              \"else\": \"selection\",\n",
    "              \"else if\": \"selection\",\n",
    "              \"try\": \"try\",\n",
    "              \"catch\": \"catch\",\n",
    "              \"finally\": \"finally\"}\n",
    "\n",
    "keywords = [\"while\", \"for\", \"do\", \"if\", \"else\", \"try\", \"catch\", \"finally\"]\n",
    "\n",
    "operators_and_symbols = ['<', '>', '=', '==', '+', '-', '*', '/',\n",
    "                         '>=', \"<=\", '{', '}', '(', ')', ',', ';', '||']\n",
    "\n",
    "\n",
    "def stringMatching(str1, str2):\n",
    "    # str1, str2 = \"\", \"\"\n",
    "\n",
    "    # for ele in num1:\n",
    "    #     str1 += ele\n",
    "    # for ele in num2:\n",
    "    #     str2 += ele\n",
    "\n",
    "    similarity = fuzz.ratio(str1, str2)\n",
    "    # print(str1, str2, similarity)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# def lcs(num1, num2, m, n):\n",
    "#     dp = []\n",
    "#     for _ in range(m+1):\n",
    "#         dp.append([])\n",
    "#         for __ in range(n+1):\n",
    "#             dp[-1].append(0)\n",
    "\n",
    "#     for i in range(m+1):\n",
    "#         for j in range(n+1):\n",
    "#             if(i == 0 or j == 0):\n",
    "#                 dp[i][j] = 0\n",
    "\n",
    "#             elif(num1[i-1] == num2[j-1]):\n",
    "#                 dp[i][j] = dp[i-1][j-1] + 1\n",
    "\n",
    "#             else:\n",
    "#                 dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
    "\n",
    "#     return dp[m][n]\n",
    "\n",
    "def checkForParenthesis(method_lines, lst_line, i):\n",
    "    assert(len(method_lines) > 0)\n",
    "    if '{' in lst_line:\n",
    "        return method_lines\n",
    "\n",
    "    else:\n",
    "        k = i+1\n",
    "        for next_line in method_lines[i+1:]:\n",
    "            next_line = next_line.strip()\n",
    "            #print(k, next_line)\n",
    "            if(len(next_line) > 0):\n",
    "                if '{' == next_line[0]:\n",
    "                    return method_lines\n",
    "                else:\n",
    "                    #print(next_line, \"++++++\" + next_line[-1] + \"+++++\")\n",
    "                    if ';' == next_line[-1]:\n",
    "                        #print(\"========\", next_line, method_lines[k])\n",
    "                        method_lines[k] = method_lines[k] + \" } \"\n",
    "                        method_lines[i] = method_lines[i] + \" { \"\n",
    "                        return method_lines\n",
    "            k += 1\n",
    "    return method_lines\n",
    "\n",
    "\n",
    "def parenthesisBalancer(method_lines):\n",
    "    for i, line in enumerate(method_lines):\n",
    "        # print(line)\n",
    "        lst_line = line.split()\n",
    "\n",
    "        for j, unit in enumerate(lst_line):\n",
    "            if unit in keywords:\n",
    "                #print(j, unit)\n",
    "                method_lines = checkForParenthesis(\n",
    "                    method_lines, lst_line, i)\n",
    "                # print(method_lines)\n",
    "\n",
    "    # for line in method_lines:\n",
    "    #     print(line)\n",
    "    return method_lines\n",
    "\n",
    "\n",
    "def getSimilarity(m1_v_scope=[], m1_mc_scope=[], m2_v_scope=[], m2_mc_scope=[], clonesInfo=[]):\n",
    "    #m1_v_scope = [[\"n\", \"1global 2iteration 1global\"], [\"temp\",]]\n",
    "    dataFlowSimilaritythreshold = 0.95\n",
    "    clone_count_variables, total_count_variables = 0, max(\n",
    "        len(m1_v_scope), len(m2_v_scope))\n",
    "    clone_count_method_calls, total_count_method_calls = 0, max(\n",
    "        len(m1_mc_scope), len(m2_mc_scope))\n",
    "\n",
    "    comparison_len_variables = min(len(m1_v_scope), len(m2_v_scope))\n",
    "    comparison_len_method_calls = min(len(m1_mc_scope), len(m2_mc_scope))\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(m1_v_scope) and j < len(m2_v_scope):\n",
    "        v_len1 = len(m1_v_scope[i][1].split())\n",
    "        v_len2 = len(m2_v_scope[j][1].split())\n",
    "\n",
    "        # if(v_len1 == 0 or v_len2 == 0):\n",
    "        # [[\"temp\", \"1global 2selection\"]]\n",
    "        #if min(v_len1, v_len2) / max(v_len1, v_len2) >= Config.dataFlowSimilaritythreshold:\n",
    "        if max(v_len1, v_len2) > 0:\n",
    "          if min(v_len1, v_len2) / max(v_len1, v_len2) >= dataFlowSimilaritythreshold:\n",
    "            similarity = stringMatching(m1_v_scope[i][1], m2_v_scope[j][1])\n",
    "\n",
    "            #if(similarity >= Config.dataFlowSimilaritythreshold):\n",
    "            if(similarity >= dataFlowSimilaritythreshold):\n",
    "                clone_count_variables += 1\n",
    "\n",
    "            i += 1\n",
    "            j += 1\n",
    "          elif v_len1 > v_len2:\n",
    "            i += 1\n",
    "          else:\n",
    "            j += 1\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(m1_mc_scope) and j < len(m2_mc_scope):\n",
    "\n",
    "        mc_len1 = len(m1_mc_scope[i][1].split())\n",
    "        mc_len2 = len(m2_mc_scope[j][1].split())\n",
    "\n",
    "        #if min(mc_len1, mc_len2) / max(mc_len1, mc_len2) >= Config.dataFlowSimilaritythreshold:\n",
    "        if max(mc_len1, mc_len2) > 0:\n",
    "          if min(mc_len1, mc_len2) / max(mc_len1, mc_len2) >= dataFlowSimilaritythreshold:\n",
    "            similarity = stringMatching(m1_mc_scope[i][1], m2_mc_scope[j][1])\n",
    "            if similarity >= dataFlowSimilaritythreshold:\n",
    "            #if similarity >= Config.dataFlowSimilaritythreshold:\n",
    "                clone_count_method_calls += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "          elif mc_len1 > mc_len2:\n",
    "            i += 1\n",
    "          else:\n",
    "            j += 1\n",
    "\n",
    "    similarityVariables = clone_count_variables / \\\n",
    "        total_count_variables if total_count_variables != 0 else 1\n",
    "    similarityMethods = clone_count_method_calls / \\\n",
    "        total_count_method_calls if total_count_method_calls != 0 else 1\n",
    "\n",
    "    return similarityVariables, similarityMethods\n",
    "\n",
    "\n",
    "def dataFlowGenerator(method_lines, identifiers, method_calls, file_info):\n",
    "    #print(\"identfiers \", identifiers)\n",
    "    identifier_scope = [[identifiers[i], \"\"] for i in range(len(identifiers))]\n",
    "    method_calls_scope = [[method_calls[i], \"\"]\n",
    "                          for i in range(len(method_calls))]\n",
    "\n",
    "    assert(len(identifiers) == len(identifier_scope))\n",
    "    assert(len(method_calls) == len(method_calls_scope))\n",
    "\n",
    "    scope_stack, parenthesis_stack = [], []\n",
    "    level = 0\n",
    "    scope = \"global\"\n",
    "\n",
    "    method_lines = parenthesisBalancer(method_lines)\n",
    "    # print(Mapping.delimiters)\n",
    "    new_delimeters = Mapping.delimiters + ['.']\n",
    "    # print(new_delimeters)\n",
    "    for line in method_lines:\n",
    "        line = re.sub(r\"(\\\".*?\\\"|\\'.*?\\')\", \" STRING_LITERAL \", line)\n",
    "        regexPattern = '|'.join(map(re.escape, new_delimeters))\n",
    "        lst_line = re.sub('(?<=\\W|\\w)(' + regexPattern + ')',\n",
    "                          r' \\1 ', line).split()\n",
    "        lst_line = [unit.strip() for unit in lst_line if unit.strip() != \"\"]\n",
    "\n",
    "        for unit in lst_line:\n",
    "            unit = unit.strip()\n",
    "            unit = re.sub(r\"^[+-]?((\\d*(\\.\\d*)?)|(\\.\\d*))$\",\n",
    "                          \"INTEGER_LITERAL\", unit)\n",
    "\n",
    "            for keyword in keywords:\n",
    "                if(unit == keyword):\n",
    "                    scope = cf_mapping[keyword]\n",
    "                    break\n",
    "\n",
    "            if unit == '{':\n",
    "                scope_stack.append(scope)\n",
    "                parenthesis_stack.append('{')\n",
    "                level += 1\n",
    "\n",
    "            if unit == '}':\n",
    "                # print(scope_stack)\n",
    "                # print(parenthesis_stack)\n",
    "                if(len(scope_stack)):\n",
    "                    scope_stack.pop()\n",
    "                if(len(scope_stack) > 0):\n",
    "                    scope = scope_stack[-1]\n",
    "                if(len(parenthesis_stack) > 0):\n",
    "                    parenthesis_stack.pop()\n",
    "                level -= 1\n",
    "\n",
    "            for identifier in identifiers:\n",
    "                if(identifier == unit):\n",
    "                    index = identifiers.index(identifier)\n",
    "\n",
    "                    # if(len(identifier_scope[index]) == 0):\n",
    "                    #     identifier_scope[index].append(identifier)\n",
    "                    #     identifier_scope[index].append(str(level) + scope)\n",
    "\n",
    "                    # else:\n",
    "                    identifier_scope[index][1] = identifier_scope[index][1] + \\\n",
    "                        \" \" + str(level) + scope\n",
    "\n",
    "            for method_call in method_calls:\n",
    "                if(method_call == unit):\n",
    "                    index = method_calls.index(method_call)\n",
    "\n",
    "                    # if(len(method_calls_scope[index]) == 0):\n",
    "                    #     method_calls_scope[index].append(method_call)\n",
    "                    #     method_calls_scope[index].append(str(level) + scope)\n",
    "\n",
    "                    # else:\n",
    "                    method_calls_scope[index][1] = method_calls_scope[index][1] + \" \" + str(\n",
    "                        level) + scope\n",
    "\n",
    "    return identifier_scope, method_calls_scope\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e254a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywordsList = \"\"\"abstract continue for new switch assert default goto \n",
    "package synchronized boolean do if private this break double implements \n",
    "protected throw byte import public throws case enum\tinstanceof return \n",
    "transient catch extends int short try char final interface static void\n",
    "class finally long strictfp volatile const float native super while String\n",
    "STRING_LITERAL INTEGER_LITERAL\n",
    "\"\"\".split()\n",
    "mapping = { keywordsList[i] : \"TOKEN\" + str(i) for i in range(0, len(keywordsList) ) }\n",
    "keywords = {keywordsList[i]: keywordsList[i]\n",
    "            for i in range(0, len(keywordsList))}\n",
    "symbols = [\"\", \"+\", \"-\", \"*\", \"/\", \" \", \"{\", \"}\", \";\", \":\", \".\",\n",
    "           \"\\t\", \"\\n\", \",\", \"(\", \")\", \"[\", \"]\", \"=\", \">\", \"<\", \" \", \"!\", \"\\\\\", \"|\", \"&\", \"%\", \"^\", \"~\", \"`\", \"?\"]\n",
    "delimiters = [\"+\", \"-\", \"*\", \"/\", \"{\", \"}\", \";\", \"\\t\",\n",
    "              \":\", \",\", \"(\", \")\", \"[\", \"]\", \"=\", \">\", \"<\", \"!\", \"\\\\\", \"|\", \"&\", \"%\", '^', \"~\", \"`\", \"?\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee133076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all file info from folder\n",
      "Extracting methods from files\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m allFilesData\u001b[38;5;241m=\u001b[39m data_extraction\u001b[38;5;241m.\u001b[39mgetAllFilesUsingFolderPath(dirPath)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting methods from files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m current_dataset,linesofcode,codeclonelines\u001b[38;5;241m=\u001b[39m \u001b[43mdata_extraction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractMethodsAllFiles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallFilesData\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(linesofcode,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal lines\u001b[39m\u001b[38;5;124m\"\u001b[39m,codeclonelines,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal cloned lines\u001b[39m\u001b[38;5;124m\"\u001b[39m, (codeclonelines\u001b[38;5;241m/\u001b[39mlinesofcode)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloning percentage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving to CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/CodeCloneTrackingSystem_Master_Thesis/data_extraction.py:45\u001b[0m, in \u001b[0;36mextractMethodsAllFiles\u001b[0;34m(listOfFiles)\u001b[0m\n\u001b[1;32m     42\u001b[0m         blocksSoFar \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     43\u001b[0m         allFilesMethodsBlocks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCodeBlock\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(blocksSoFar)] \u001b[38;5;241m=\u001b[39m codeBlock\n\u001b[0;32m---> 45\u001b[0m granularity \u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39mgranularity\n\u001b[1;32m     46\u001b[0m codeBlocks,codeclonelines\u001b[38;5;241m=\u001b[39mCloneDetector\u001b[38;5;241m.\u001b[39mdetectClone(allFilesMethodsBlocks)\n\u001b[1;32m     48\u001b[0m previous_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/vivekgoud/Downloads/thesis/Tracking_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/CodeCloneTrackingSystem_Master_Thesis/CloneDetector.py:82\u001b[0m, in \u001b[0;36mdetectClone\u001b[0;34m(codeBlocks)\u001b[0m\n\u001b[1;32m     77\u001b[0m candidate_method_calls_scope \u001b[38;5;241m=\u001b[39m codeCandidateBlock[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod_Calls_Scope\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# print(\"Variables Scope\", variable_scope)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# print(\"Methods Calls Scope\", method_calls_scope)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# print(\"Candidate vC\", candidate_variable_scope)\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print(\"Can MC\", candidate_method_calls_scope)\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m variableSimilarityByDataFlow, methodCallSimilarityByDataFlow \u001b[38;5;241m=\u001b[39m \u001b[43mDataFlowApproach\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetSimilarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvariable_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod_calls_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_variable_scope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcandidate_method_calls_scope\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcodeBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFileInfo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodeBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodeBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcodeCandidateBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFileInfo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodeCandidateBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mStart\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodeCandidateBlock\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEnd\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m variableSimilarityByDataFlow \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39msimilarityDataFlowThreshold \u001b[38;5;129;01mand\u001b[39;00m methodCallSimilarityByDataFlow \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Config\u001b[38;5;241m.\u001b[39msimilarityDataFlowThreshold:\n\u001b[1;32m     87\u001b[0m     codeclonelines \u001b[38;5;241m=\u001b[39m codeclonelines \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(codeCandidateBlock[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/GitHub/CodeCloneTrackingSystem_Master_Thesis/DataFlowApproach.py:111\u001b[0m, in \u001b[0;36mgetSimilarity\u001b[0;34m(m1_v_scope, m1_mc_scope, m2_v_scope, m2_mc_scope, clonesInfo)\u001b[0m\n\u001b[1;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(m1_v_scope) \u001b[38;5;129;01mand\u001b[39;00m j \u001b[38;5;241m<\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m(m2_v_scope):\n\u001b[1;32m    112\u001b[0m     v_len1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(m1_v_scope[i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())\n\u001b[1;32m    113\u001b[0m     v_len2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(m2_v_scope[j][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import GetFiles\n",
    "import data_extraction\n",
    "import CloneDetector\n",
    "#import CloneSave\n",
    "\n",
    "#import ml\n",
    "# save char2vec with diff name and load clustering model pickle file\n",
    "# allFilesData is list which have all files with specific extension\n",
    "print(\"Getting all file info from folder\")\n",
    "dirPath = \"/Users/vivekgoud/Documents/GitHub/Test_project_Codeclonetracer/onlinebookstore-J2EE/\"\n",
    "allFilesData= data_extraction.getAllFilesUsingFolderPath(dirPath)\n",
    "print(\"Extracting methods from files\")\n",
    "\n",
    "current_dataset,linesofcode,codeclonelines= data_extraction.extractMethodsAllFiles(allFilesData)\n",
    "\n",
    "\n",
    "print(linesofcode,\"total lines\",codeclonelines,\"total cloned lines\", (codeclonelines/linesofcode)*100 , \"cloning percentage\")\n",
    "print(\"Saving to CSV\")\n",
    "# CloneSave.writeToFile(codeBlocks)\n",
    "#CloneSave.writeToCSV(codeBlocks)\n",
    "\n",
    "\n",
    "#pip install python-Levenshtein\n",
    "\n",
    "#pip install pydriller\n",
    "#pip install fuzzywuzzy\n",
    "#pip install pandas\n",
    "#pip install javalang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7af422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
